{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Demo of Context-Aware Image Inpainting for Automatic Object Removal"
      ],
      "metadata": {
        "id": "3FfHTfAqRJXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download sample images and model checkpoints:\n",
        "For simplicity, only the baseline, 0.5L1 + 0.5SSIM joint reconstruction loss, and 0.3L2 + 0.7SSIM models are downloaded.\n",
        "The others can be manually downloaded from [https://drive.google.com/drive/folders/11yjGAIRzUpQH2IoSuLsKn6Vm9yr_-29h?usp=drive_link]"
      ],
      "metadata": {
        "id": "bU576eCVfQ9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, download the sample images and model checkpoints:\n",
        "!wget --no-check-certificate -L 'https://docs.google.com/uc?export=download&id=1kpu4lvoFmQffxBkfAMsMloXlxtYT-kf8' -O object_classes.txt\n",
        "!wget --no-check-certificate -L 'https://docs.google.com/uc?export=download&id=1f24sS0lYczLJ0BJzrPuELVx-_Suj6nEi&format=zip' -O sample_images.zip\n",
        "!wget --no-check-certificate -L 'https://docs.google.com/uc?export=download&id=1OVX9n70-CPWjPG_6tL_P39KgQQcHI9j_&format=zip' -O sample_images_object_removal.zip\n",
        "!unzip sample_images.zip\n",
        "!unzip sample_images_object_removal.zip\n",
        "!rm sample_images.zip\n",
        "!rm sample_images_object_removal.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34kshBj_UJH8",
        "outputId": "58ea0983-1933-4b77-dbe9-dd34852bb1c6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-30 23:25:21--  https://docs.google.com/uc?export=download&id=1kpu4lvoFmQffxBkfAMsMloXlxtYT-kf8\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.23.102, 74.125.23.113, 74.125.23.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.23.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1kpu4lvoFmQffxBkfAMsMloXlxtYT-kf8&export=download [following]\n",
            "--2024-04-30 23:25:22--  https://drive.usercontent.google.com/download?id=1kpu4lvoFmQffxBkfAMsMloXlxtYT-kf8&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 108.177.97.132, 2404:6800:4008:c00::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|108.177.97.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1532 (1.5K) [application/octet-stream]\n",
            "Saving to: ‘object_classes.txt’\n",
            "\n",
            "object_classes.txt  100%[===================>]   1.50K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-30 23:25:23 (73.6 MB/s) - ‘object_classes.txt’ saved [1532/1532]\n",
            "\n",
            "--2024-04-30 23:25:23--  https://docs.google.com/uc?export=download&id=1f24sS0lYczLJ0BJzrPuELVx-_Suj6nEi&format=zip\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.23.102, 74.125.23.113, 74.125.23.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.23.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1f24sS0lYczLJ0BJzrPuELVx-_Suj6nEi&export=download [following]\n",
            "--2024-04-30 23:25:23--  https://drive.usercontent.google.com/download?id=1f24sS0lYczLJ0BJzrPuELVx-_Suj6nEi&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 108.177.97.132, 2404:6800:4008:c00::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|108.177.97.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 294739 (288K) [application/octet-stream]\n",
            "Saving to: ‘sample_images.zip’\n",
            "\n",
            "sample_images.zip   100%[===================>] 287.83K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2024-04-30 23:25:24 (88.6 MB/s) - ‘sample_images.zip’ saved [294739/294739]\n",
            "\n",
            "--2024-04-30 23:25:24--  https://docs.google.com/uc?export=download&id=1OVX9n70-CPWjPG_6tL_P39KgQQcHI9j_&format=zip\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.23.102, 74.125.23.113, 74.125.23.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.23.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1OVX9n70-CPWjPG_6tL_P39KgQQcHI9j_&export=download [following]\n",
            "--2024-04-30 23:25:25--  https://drive.usercontent.google.com/download?id=1OVX9n70-CPWjPG_6tL_P39KgQQcHI9j_&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 108.177.97.132, 2404:6800:4008:c00::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|108.177.97.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7158400 (6.8M) [application/octet-stream]\n",
            "Saving to: ‘sample_images_object_removal.zip’\n",
            "\n",
            "sample_images_objec 100%[===================>]   6.83M  15.7MB/s    in 0.4s    \n",
            "\n",
            "2024-04-30 23:25:29 (15.7 MB/s) - ‘sample_images_object_removal.zip’ saved [7158400/7158400]\n",
            "\n",
            "Archive:  sample_images.zip\n",
            "   creating: sample_images/\n",
            "  inflating: sample_images/00006461.jpg  \n",
            "  inflating: sample_images/00001483.jpg  \n",
            "  inflating: sample_images/00009544.jpg  \n",
            "  inflating: sample_images/00000159.jpg  \n",
            "  inflating: sample_images/00006058.jpg  \n",
            "  inflating: sample_images/00007032.jpg  \n",
            "  inflating: sample_images/00004472.jpg  \n",
            "  inflating: sample_images/00009397.jpg  \n",
            "  inflating: sample_images/00004658.jpg  \n",
            "  inflating: sample_images/00003866.jpg  \n",
            "  inflating: sample_images/00007396.jpg  \n",
            "  inflating: sample_images/00001081.jpg  \n",
            "  inflating: sample_images/00000957.jpg  \n",
            "  inflating: sample_images/00000598.jpg  \n",
            "  inflating: sample_images/00005972.jpg  \n",
            "  inflating: sample_images/00004844.jpg  \n",
            "  inflating: sample_images/00004299.jpg  \n",
            "  inflating: sample_images/00001083.jpg  \n",
            "  inflating: sample_images/00000926.jpg  \n",
            "  inflating: sample_images/00001621.jpg  \n",
            "  inflating: sample_images/00002711.jpg  \n",
            "  inflating: sample_images/00001783.jpg  \n",
            "  inflating: sample_images/00001018.jpg  \n",
            "  inflating: sample_images/00000845.jpg  \n",
            "  inflating: sample_images/00008404.jpg  \n",
            "  inflating: sample_images/00000064.jpg  \n",
            "  inflating: sample_images/00000515.jpg  \n",
            "  inflating: sample_images/00002299.jpg  \n",
            "  inflating: sample_images/00002919.jpg  \n",
            "  inflating: sample_images/00004561.jpg  \n",
            "  inflating: sample_images/00004560.jpg  \n",
            "  inflating: sample_images/00000882.jpg  \n",
            "  inflating: sample_images/00001585.jpg  \n",
            "  inflating: sample_images/00001195.jpg  \n",
            "  inflating: sample_images/00005127.jpg  \n",
            "  inflating: sample_images/00002933.jpg  \n",
            "  inflating: sample_images/00001751.jpg  \n",
            "  inflating: sample_images/00001210.jpg  \n",
            "  inflating: sample_images/00002650.jpg  \n",
            "  inflating: sample_images/00002283.jpg  \n",
            "  inflating: sample_images/00003823.jpg  \n",
            "  inflating: sample_images/00000119.jpg  \n",
            "  inflating: sample_images/00000280.jpg  \n",
            "  inflating: sample_images/00000243.jpg  \n",
            "  inflating: sample_images/00002681.jpg  \n",
            "  inflating: sample_images/00006949.jpg  \n",
            "  inflating: sample_images/00004379.jpg  \n",
            "  inflating: sample_images/00001639.jpg  \n",
            "  inflating: sample_images/00002047.jpg  \n",
            "  inflating: sample_images/00003114.jpg  \n",
            "  inflating: sample_images/00002752.jpg  \n",
            "  inflating: sample_images/00000597.jpg  \n",
            "  inflating: sample_images/00008487.jpg  \n",
            "  inflating: sample_images/00007373.jpg  \n",
            "  inflating: sample_images/00000146.jpg  \n",
            "  inflating: sample_images/00006286.jpg  \n",
            "  inflating: sample_images/00004047.jpg  \n",
            "  inflating: sample_images/00002350.jpg  \n",
            "  inflating: sample_images/00001472.jpg  \n",
            "  inflating: sample_images/00009833.jpg  \n",
            "  inflating: sample_images/00000618.jpg  \n",
            "  inflating: sample_images/00004911.jpg  \n",
            "  inflating: sample_images/00001840.jpg  \n",
            "  inflating: sample_images/00002781.jpg  \n",
            "Archive:  sample_images_object_removal.zip\n",
            "   creating: sample_images_object_removal/\n",
            "  inflating: sample_images_object_removal/giraffe_zebra.jpg  \n",
            "  inflating: sample_images_object_removal/beach.jpg  \n",
            "  inflating: sample_images_object_removal/tree.jpg  \n",
            "  inflating: sample_images_object_removal/cliff.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model checkpoints:\n",
        "!pip install gdown\n",
        "!gdown 1mmDswcNnt6vu3jbO7hgyhV_-ts8Rtm6i # Baseline\n",
        "# !gdown 1SbsNVqOEcajMQoJq0-7_Y1jQa1i4uVfT # L1 + SSIM Joint Reconstruction\n",
        "!gdown 1e1NRuB2urJKHQS8pQLizTJ-t8gymnj4R # 0.5L1 + 0.5SSIM Joint Reconstruction\n",
        "!gdown 1z8S2KSPWHrqrexGD1DdHkD1g2cdlx_aM # L2 + SSIM Joint Reconstruction\n",
        "!mkdir Checkpoints\n",
        "!mv /content/model_baseline_epoch_39.pth /content/Checkpoints/model_baseline_epoch_39.pth\n",
        "# !mv /content/model_L1_SSIM_epoch_39.pth /content/Checkpoints/model_L1_SSIM_epoch_39.pth\n",
        "!mv /content/model_0.5L1_0.5SSIM_epoch_39.pth /content/Checkpoints/model_0.5L1_0.5SSIM_epoch_39.pth\n",
        "!mv /content/model_L2_SSIM_epoch_39.pth /content/Checkpoints/model_L2_SSIM_epoch_39.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-UcUC0iihut",
        "outputId": "e1bc7d0d-648c-4e1b-ee3c-f364734d8416"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1mmDswcNnt6vu3jbO7hgyhV_-ts8Rtm6i\n",
            "From (redirected): https://drive.google.com/uc?id=1mmDswcNnt6vu3jbO7hgyhV_-ts8Rtm6i&confirm=t&uuid=0349480c-22fe-4e9c-a71b-ddd35b3a6a03\n",
            "To: /content/model_baseline_epoch_39.pth\n",
            "100% 504M/504M [00:29<00:00, 16.9MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1e1NRuB2urJKHQS8pQLizTJ-t8gymnj4R\n",
            "From (redirected): https://drive.google.com/uc?id=1e1NRuB2urJKHQS8pQLizTJ-t8gymnj4R&confirm=t&uuid=aa721664-afbc-4811-a7d0-4037622ff0cc\n",
            "To: /content/model_0.5L1_0.5SSIM_epoch_39.pth\n",
            "100% 504M/504M [00:13<00:00, 38.0MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1z8S2KSPWHrqrexGD1DdHkD1g2cdlx_aM\n",
            "From (redirected): https://drive.google.com/uc?id=1z8S2KSPWHrqrexGD1DdHkD1g2cdlx_aM&confirm=t&uuid=990a0076-a084-463b-8353-2c85f7bfee2a\n",
            "To: /content/model_L2_SSIM_epoch_39.pth\n",
            "100% 504M/504M [00:13<00:00, 37.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Code\n",
        "The following code is used to create a test dataloader for evaluating entire folders of images at once:"
      ],
      "metadata": {
        "id": "8ZWBUMLGRV15"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EXzvaXJ9Oy91"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, transforms_=None, img_size=128, mask_size=64, mode=\"train\"):\n",
        "        self.transform = transforms.Compose(transforms_)\n",
        "        self.img_size = img_size\n",
        "        self.mask_size = mask_size\n",
        "        self.mode = mode\n",
        "        self.files = sorted(glob.glob(\"%s/**/*.jpg\" % root, recursive=True))\n",
        "        # self.files = sorted(glob.glob(\"{}/**/*.jpg\".format(root), recursive=True))\n",
        "\n",
        "    def apply_random_mask(self, img):\n",
        "        \"\"\"Randomly masks image\"\"\"\n",
        "        y1, x1 = np.random.randint(0, self.img_size - self.mask_size, 2)\n",
        "        y2, x2 = y1 + self.mask_size, x1 + self.mask_size\n",
        "        masked_part = img[:, y1:y2, x1:x2]\n",
        "        masked_img = img.clone()\n",
        "        masked_img[:, y1:y2, x1:x2] = 1\n",
        "\n",
        "        return masked_img, masked_part, x1, y1\n",
        "\n",
        "    def apply_center_mask(self, img):\n",
        "        \"\"\"Mask center part of image\"\"\"\n",
        "        # Get upper-left pixel coordinate\n",
        "        i = (self.img_size - self.mask_size) // 2\n",
        "        masked_img = img.clone()\n",
        "        masked_img[:, i : i + self.mask_size, i : i + self.mask_size] = 1\n",
        "\n",
        "        return masked_img, i\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.files[index % len(self.files)])\n",
        "        img = self.transform(img)\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            # For training data perform random mask\n",
        "            masked_img, aux, x1, y1 = self.apply_random_mask(img)\n",
        "            return img, masked_img, aux, x1, y1 # aux = masked_part\n",
        "        else:\n",
        "            # For test data mask the center of the image\n",
        "            masked_img, aux = self.apply_center_mask(img)\n",
        "            return img, masked_img, aux\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context Encoders Model Code"
      ],
      "metadata": {
        "id": "9qV2AHR9RndO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "# ContextGenerator:\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels=3, bottleneck_dim=4000): # bottleneck_dim = 2048\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # Basic Block:\n",
        "        def downsample(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Conv2d(in_feat, out_feat, kernel_size=4, stride=2, padding=1)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2)) # inplace=True\n",
        "            return layers\n",
        "\n",
        "        # Transpose Block:\n",
        "        def upsample(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.ConvTranspose2d(in_feat, out_feat, kernel_size=4, stride=2, padding=1)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n",
        "            layers.append(nn.ReLU())\n",
        "            return layers\n",
        "\n",
        "        # Must add final layer *upsample(64, 64) to resize to 128\n",
        "        self.ContextEncoder = nn.Sequential(\n",
        "            *downsample(channels, 64, normalize=False),\n",
        "            *downsample(64, 64),    # if img size = 128\n",
        "            *downsample(64, 128),\n",
        "            *downsample(128, 256),\n",
        "            *downsample(256, 512),\n",
        "            nn.Conv2d(512, bottleneck_dim, 1) # try kernel_size=4 (last arg)\n",
        "        )\n",
        "        self.ContextDecoder = nn.Sequential( # Add nn.BatchNorm2d(bottleneck_dim) and nn.leakyReLU(0.2) before upsamples?\n",
        "            *upsample(bottleneck_dim, 512),\n",
        "            *upsample(512, 256),\n",
        "            *upsample(256, 128),\n",
        "            *upsample(128, 64),\n",
        "            *upsample(64, 64),    # needed to resize back to 128 x 128\n",
        "            nn.Conv2d(64, channels, 3, 1, 1), # try kernel_size=4, stride=2, padding=1, bias=False\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ContextEncoder(x)\n",
        "        return self.ContextDecoder(x)\n",
        "\n",
        "# ContextDiscriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, stride, normalize):\n",
        "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n",
        "            if normalize:\n",
        "                layers.append(nn.InstanceNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        layers = []\n",
        "        in_filters = channels\n",
        "        for out_filters, stride, normalize in [(64, 2, False), (128, 2, True), (256, 2, True), (512, 1, True)]:\n",
        "            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize))\n",
        "            in_filters = out_filters\n",
        "\n",
        "        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        # Add final conv2d and sigmoid?\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img)"
      ],
      "metadata": {
        "id": "y2IQsupWRn0o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependency for YOLOv8 pre-trained model"
      ],
      "metadata": {
        "id": "hi3-cVjnRyCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "3HtNJrH4RyJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c7ea9d-2548-4240-83ad-860c808eb543"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.5-py3-none-any.whl (754 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.0/755.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.17.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Collecting thop>=0.1.1 (from ultralytics)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, thop, ultralytics\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 thop-0.1.1.post2209072238 ultralytics-8.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model for Object Segmentation and Mask Extraction:"
      ],
      "metadata": {
        "id": "DzKR2umBSMwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "class Segmentor():\n",
        "  def __init__(self, input_image_path):\n",
        "    self.model = YOLO('yolov8m-seg.pt')\n",
        "    self.input_image_path = input_image_path\n",
        "    self.image = np.array(Image.open(input_image_path))\n",
        "    self.load_config()\n",
        "\n",
        "  def get_results(self, input_file_path):\n",
        "    return self.model(input_file_path)\n",
        "\n",
        "  def save_image(self, img, output_path='/', ):\n",
        "    cv2.imwrite(output_path, img.cpu().numpy())\n",
        "\n",
        "  def display_image(self, img, title=''):\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.imshow(img, interpolation='none')\n",
        "    plt.show()\n",
        "\n",
        "  def display_mask_overlay(self, img, mask, title=''):\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.imshow(img, 'gray', interpolation='none')\n",
        "    mask = mask.cpu().numpy()\n",
        "    mask = np.ma.masked_where(mask == 0, mask)\n",
        "    plt.imshow(mask, 'jet', interpolation='none', alpha=0.5)\n",
        "    plt.show()\n",
        "\n",
        "  def display_and_save_mask_overlay(self, img, mask, title='', output_path='/'):\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.imshow(img, 'gray', interpolation='none')\n",
        "    mask = mask.cpu().numpy()\n",
        "    mask = np.ma.masked_where(mask == 0, mask)\n",
        "    plt.imshow(mask, 'jet', interpolation='none', alpha=0.5)\n",
        "    plt.savefig(output_path)\n",
        "    plt.show()\n",
        "\n",
        "  def display_and_save_image(self, img, title='', output_path='/'):\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.imshow(img, interpolation='none')\n",
        "    plt.savefig(output_path)\n",
        "    plt.show()\n",
        "\n",
        "  #\n",
        "  def run_detection_segmentation(self):\n",
        "    self.display_image(self.image, title=\"Original Image\")\n",
        "\n",
        "    results = self.get_results(self.input_image_path)\n",
        "    result = results[0] # since we only pass a single image to segment, there is only one result\n",
        "\n",
        "    pred_array = result.plot()  # BGR numpy array of predictions\n",
        "    pred_img = Image.fromarray(pred_array[..., ::-1])  # Convert to RGB PIL image\n",
        "\n",
        "    #self.display_and_save_image(pred_img, output_path='/content/predictions.jpg')\n",
        "    self.display_image(pred_img, title=\"Predictions\")\n",
        "\n",
        "    # Get the lists of masks and bounding boxes:\n",
        "    if result.masks is None:\n",
        "        return None, None, None\n",
        "    masks = result.masks.data\n",
        "    boxes = result.boxes.data\n",
        "\n",
        "    # Save the masks and boxes:\n",
        "    self.masks = masks\n",
        "    self.boxes = boxes\n",
        "    self.detected_objects = boxes[:, 5]\n",
        "\n",
        "    detected_objects = boxes[:, 5]\n",
        "\n",
        "    # Extract a mask with all detected objects:\n",
        "    obj_indices = torch.where(detected_objects != -1)\n",
        "    obj_masks = masks[obj_indices]\n",
        "    obj_mask = torch.any(obj_masks, dim=0).int() * 255\n",
        "    self.save_image(obj_mask, '/content/all-detected-objects-masks.jpg')\n",
        "\n",
        "    return masks, detected_objects, pred_img\n",
        "\n",
        "\n",
        "  # Get the mask that includes specified objects:\n",
        "  def get_mask(self, objects=None):\n",
        "    if objects is None:\n",
        "        objects = self.OBJECTS\n",
        "\n",
        "    masks, detected_objects, predictions_img = self.run_detection_segmentation()\n",
        "    if masks is None:\n",
        "        return None\n",
        "\n",
        "    # Extract a single mask that contains all segmentations of specified object types:\n",
        "    object_indices = []\n",
        "\n",
        "    # Mask for all instances of an object type:\n",
        "    # for i, seg_class in enumerate(seg_classes):\n",
        "    for id in objects:\n",
        "\n",
        "        obj_indices = torch.where(detected_objects == id) # clss - id\n",
        "        object_indices.append(obj_indices[0])\n",
        "        obj_masks = masks[obj_indices]\n",
        "        obj_mask = torch.any(obj_masks, dim=0).int() * 255 # Tensor\n",
        "\n",
        "        #self.save_image(obj_mask, str(f'/content/object_class{id}_mask.jpg'))\n",
        "\n",
        "        # Resize mask to image size:\n",
        "        # print(\"image shape\", img.shape)\n",
        "        # image_height, image_width = img.shape[:2]\n",
        "        # obj_mask = cv2.resize(np.array(obj_mask, dtype='uint8'), (image_width, image_height), interpolation=cv2.INTER_CUBIC)\n",
        "        # OR Resize image to mask size (works better?):\n",
        "        mask_height, mask_width = obj_mask.shape[:2]\n",
        "        resized_img = cv2.resize(self.image, (mask_width, mask_height), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        # Convert into a mask that can be directly applied to an image:\n",
        "        obj_mask = obj_mask.cpu().numpy()\n",
        "        actual_mask = np.ma.masked_where(obj_mask == 0, obj_mask)\n",
        "\n",
        "        # Plot the input image, the mask overlayed on the image, and the image after the mask is applied:\n",
        "        fig, ax = plt.subplots(nrows=1, ncols=3, tight_layout=True)\n",
        "        ax[0].set_title(\"Original Image\")\n",
        "        ax[0].axis('off')\n",
        "        ax[0].imshow(self.image, 'gray', interpolation='none')\n",
        "\n",
        "        # Mask overlay:\n",
        "        ax[1].set_title(str(f'{id} Mask Overlay'))\n",
        "        ax[1].axis('off')\n",
        "        ax[1].imshow(resized_img, 'gray', interpolation='none')\n",
        "        ax[1].imshow(obj_mask, 'jet', interpolation='none', alpha=0.5)\n",
        "\n",
        "        # Mask applied to image:\n",
        "        ax[2].set_title(str(f'{id} Mask Directly Applied'))\n",
        "        ax[2].axis('off')\n",
        "        ax[2].imshow(resized_img, 'gray', interpolation='none')\n",
        "        ax[2].imshow(actual_mask, 'jet', interpolation='none', alpha=0.5)\n",
        "        # plt.savefig(str(f'/content/object_class{id}_applied_mask.jpg'), bbox_inches='tight', pad_inches = 0)\n",
        "        fig.show()\n",
        "\n",
        "    # Extract a single mask that contains all segmentations of specified object types:\n",
        "    object_indices = torch.cat(object_indices, dim=0)\n",
        "    object_masks = masks[object_indices]\n",
        "    object_mask = torch.any(object_masks, dim=0).int() * 255\n",
        "    self.save_image(object_mask, '/content/objects-to-remove-masks.jpg')\n",
        "\n",
        "    self.display_image(object_mask.cpu().numpy(), title=\"Mask of Objects to Remove\")\n",
        "    self.display_and_save_mask_overlay(resized_img, object_mask, title=\"Final Mask Applied to Image\", output_path='/content/object_removal_segmentation.jpg')\n",
        "\n",
        "    # Plot the input image, the object detection predictions, and the extracted mask:\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=3, tight_layout=True)\n",
        "    ax[0].set_title(\"Original Image\")\n",
        "    ax[0].axis('off')\n",
        "    ax[0].imshow(self.image, cmap='gray', interpolation='none')\n",
        "\n",
        "    # Mask overlay:\n",
        "    ax[1].set_title(\"Predictions\")\n",
        "    ax[1].axis('off')\n",
        "    ax[1].imshow(predictions_img, cmap='gray', interpolation='none')\n",
        "\n",
        "    # Mask applied to image:\n",
        "    ax[2].set_title(\"Masks\")\n",
        "    ax[2].axis('off')\n",
        "    ax[2].imshow(object_mask.cpu().numpy(), cmap='gray', interpolation='none')\n",
        "    plt.savefig(str(f'/content/predictions_and_mask.jpg'), bbox_inches='tight', pad_inches = 0)\n",
        "    fig.show()\n",
        "\n",
        "    return object_mask\n",
        "\n",
        "  def load_config(self):\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--remove', nargs= '*' ,type=int, help='objects to remove')\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    self.OBJECTS = args.remove if args.remove is not None else [0] # default to removing people"
      ],
      "metadata": {
        "id": "uc23erRLSM0S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependency for SSIM and MS-SSIM metrics:"
      ],
      "metadata": {
        "id": "d0bYHNM_SabR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpz9qNxwSrXv",
        "outputId": "22c99790-f409-4e60-ac63-d0b378315e3a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.2 torchmetrics-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Code:\n",
        "Instructions:\n",
        "1. Evaluate on folder of images:\n",
        "\n",
        "Specify --image_folder_path in load_args() by replacing default=''.\n",
        "\n",
        "2. Evaluate on single image:\n",
        "\n",
        "Specify --image_path in load_args() by replacing default='' AND set --image_folder_path default to ''.\n",
        "\n",
        "3. Evaluate single image with object removal:\n",
        "\n",
        "Same as (2), but specify --remove in load_args by adding default=[list of object ids]. See object_classes.txt for a list of supported objects and their ids."
      ],
      "metadata": {
        "id": "poq98D3wSrc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "def load_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # Specify path to image folder:\n",
        "    parser.add_argument(\"--image_folder_path\", type=str, default='/content/sample_images', help=\"Path to folder of input images\")\n",
        "    # parser.add_argument(\"--image_folder_path\", type=str, default='', help=\"Path to folder of input images\")\n",
        "\n",
        "    # Specify path to single image (make sure default='' for --image_folder_path):\n",
        "    parser.add_argument(\"--image_path\", type=str, default='/content/sample_images_object_removal/beach.jpg', help=\"Path to image\")\n",
        "    # parser.add_argument(\"--image_path\", type=str, default='/content/sample_images_object_removal/cliff.jpg', help=\"Path to image\")\n",
        "    # parser.add_argument(\"--image_path\", type=str, default='/content/sample_images_object_removal/giraffe_zebra.jpg', help=\"Path to image\")\n",
        "    # parser.add_argument(\"--image_path\", type=str, default='/content/sample_images_object_removal/tree.jpg', help=\"Path to image\")\n",
        "\n",
        "    # Choose which model to use:\n",
        "    # parser.add_argument(\"--model_checkpoint\", type=str, default='/content/Checkpoints/model_baseline_epoch_39.pth', help=\"name of the model checkpoint file to use for evaluation\")\n",
        "    parser.add_argument(\"--model_checkpoint\", type=str, default='/content/Checkpoints/model_0.5L1_0.5SSIM_epoch_39.pth', help=\"name of the model checkpoint file to use for evaluation\")\n",
        "    # parser.add_argument(\"--model_checkpoint\", type=str, default='/content/Checkpoints/model_L2_SSIM_epoch_39.pth', help=\"name of the model checkpoint file to use for evaluation\")\n",
        "\n",
        "    # Specify which objects to automatically remove:\n",
        "    #parser.add_argument('--remove', nargs= '*' ,type=int, help='objects to remove')\n",
        "    parser.add_argument('--remove', default=[0, 2], nargs= '*' ,type=int, help='objects to remove')\n",
        "\n",
        "    # parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches (grid when evaluating folder of images)\")\n",
        "    # parser.add_argument(\"--num_cols\", type=int, default=8, help=\"Number of images per column in output grid (use with folder of images)\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=-1, help=\"size of the batches (grid when evaluating folder of images)\")\n",
        "    parser.add_argument(\"--num_cols\", type=int, default=-1, help=\"Number of images per column in output grid (use with folder of images)\")\n",
        "\n",
        "    # Do not change img_size, mask_size, or channels:\n",
        "    parser.add_argument(\"--img_size\", type=int, default=128, help=\"size of each image dimension\")\n",
        "    parser.add_argument(\"--mask_size\", type=int, default=64, help=\"size of random mask\")\n",
        "    parser.add_argument(\"--channels\", type=int, default=3, help=\"Number of image channels\")\n",
        "    args = parser.parse_args(args=[])\n",
        "    print(args)\n",
        "    return args\n",
        "\n",
        "args = load_args()\n",
        "\n",
        "os.makedirs(\"inpainting_results\", exist_ok=True)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "# Load the generator from the given checkpoint:\n",
        "generator = Generator(channels=args.channels)\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    checkpoint = torch.load(args.model_checkpoint)\n",
        "    generator.load_state_dict(checkpoint[\"generator_state_dict\"])\n",
        "else:\n",
        "    # If using the cpu, specify map_location to load tensors in cpu form:\n",
        "    checkpoint = torch.load(args.model_checkpoint, map_location=torch.device('cpu'))\n",
        "    generator.load_state_dict(checkpoint[\"generator_state_dict\"])\n",
        "generator.eval()\n",
        "\n",
        "# Create image and mask input transformations:\n",
        "mask_transforms_ = [\n",
        "    transforms.Resize((args.img_size, args.img_size), Image.BICUBIC),\n",
        "    transforms.ToTensor()\n",
        "]\n",
        "mask_transforms = transforms.Compose(mask_transforms_) # Callable transformation\n",
        "eval_transforms_ = [\n",
        "    transforms.Resize((args.img_size, args.img_size), Image.BICUBIC),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "]\n",
        "eval_transforms = transforms.Compose(eval_transforms_) # Callable transformation\n",
        "\n",
        "# Define loss functions:\n",
        "pixelwise_loss_L2 = torch.nn.MSELoss() # L2 Loss\n",
        "pixelwise_loss_L1 = torch.nn.L1Loss()  # L1 loss\n",
        "loss_functions = [(\"L2 Loss\", torch.nn.MSELoss()), (\"L1 Loss\", torch.nn.L1Loss())]\n",
        "\n",
        "# PSNR = 10 * log_10( (max_dtype_value)^2 / MSE(gen_img, img))\n",
        "def get_psnr(gen_img, img):\n",
        "    mse = torch.mean((gen_img - img) ** 2)\n",
        "    if(mse == 0):  # Perfect reconstruction\n",
        "        return 100\n",
        "    psnr = 10 * torch.log10(1.0 / mse)\n",
        "    return psnr\n",
        "\n",
        "# SSIM:\n",
        "def get_ssim(gen_img, img):\n",
        "    from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
        "    SSIM = StructuralSimilarityIndexMeasure(data_range=1.0) # images are normalized\n",
        "    return SSIM(gen_img, img)\n",
        "\n",
        "# Multi-scale SSIM (currently gives NaN, so it is not in use):\n",
        "def get_ms_ssim(gen_img, img):\n",
        "    from torchmetrics.image import MultiScaleStructuralSimilarityIndexMeasure\n",
        "    # Default kernel_size=11 is too big for 128x128 images\n",
        "    MS_SSIM = MultiScaleStructuralSimilarityIndexMeasure(data_range=1.0, kernel_size=7, normalize=None)\n",
        "    return MS_SSIM(gen_img, img)\n",
        "\n",
        "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
        "SSIM_LossFn = StructuralSimilarityIndexMeasure(data_range=1.0) # images are normalized\n",
        "if cuda:\n",
        "    SSIM_LossFn.cuda()\n",
        "\n",
        "# By default, apply center masking. If objects to remove are specified, detect the given objects and delete them\n",
        "if args.image_folder_path: # Works for folder of jpgs (also recursively takes images in subfolders)\n",
        "    # Get number of images in input folder:\n",
        "    num_images = len(glob.glob(\"%s/**/*.jpg\" % args.image_folder_path, recursive=True))\n",
        "    print(\"Number of input images:\", num_images)\n",
        "\n",
        "    if args.batch_size == -1:\n",
        "        args.batch_size = num_images\n",
        "        print(\"==> Batch size not specified. Using total number of images: {}\".format(num_images))\n",
        "\n",
        "    # If the number of rows is not specified, take the sqrt of the number of images\n",
        "    # to produce a square grid\n",
        "    if args.num_cols == -1:\n",
        "        import math\n",
        "        args.num_cols = math.ceil(math.sqrt(args.batch_size))\n",
        "        print(\"==> Columns not specified. Using ceil(sqrt(num_images)) = {}\".format(args.num_cols))\n",
        "\n",
        "    # Load the input image folder:\n",
        "    test_dataloader = DataLoader(\n",
        "      ImageDataset(args.image_folder_path, transforms_=eval_transforms_, mode=\"val\"),\n",
        "      batch_size=args.batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=1,\n",
        "    )\n",
        "\n",
        "    # Initialize loss counters\n",
        "    total_L2 = 0\n",
        "    total_L1 = 0\n",
        "    total_PSNR = 0\n",
        "    total_SSIM = 0\n",
        "\n",
        "    for batch_idx, (samples, masked_samples, i) in enumerate(test_dataloader):\n",
        "        samples = Variable(samples.type(Tensor))\n",
        "        masked_samples = Variable(masked_samples.type(Tensor))\n",
        "        i = i[0].item()  # Upper-left coordinate of mask\n",
        "\n",
        "        # Generate inpainted images:\n",
        "        gen_img = generator(masked_samples)\n",
        "        gen_mask = gen_img[:, :, i : i + args.mask_size, i : i + args.mask_size]\n",
        "        filled_samples = masked_samples.clone()\n",
        "        filled_samples[:, :, i : i + args.mask_size, i : i + args.mask_size] = gen_mask\n",
        "\n",
        "        '''Save Output Images (currently, for first batch):'''\n",
        "        if batch_idx == 0:\n",
        "            ''' Save\n",
        "              (1) Grid of input, masked input, and inpainted output\n",
        "              (2) Grid of input, masked input, generated images, and inpainted output\n",
        "              (3) Grid of input images\n",
        "              (4) Grid of masked input images\n",
        "              (5) Grid of the resulting inpainted images\n",
        "            '''\n",
        "            # Save grid of input, masked input, and inpainted output (originally, nrow=8):\n",
        "            sample = torch.cat((samples.data, masked_samples.data, filled_samples.data), -2)\n",
        "            save_image(sample, \"inpainting_results/inpainted_grid.png\", nrow=args.num_cols, normalize=True)\n",
        "            # Save grid of input, masked input, generated image, and inpainted output:\n",
        "            sample = torch.cat((samples.data, masked_samples.data, gen_img.data, filled_samples.data), -2)\n",
        "            save_image(sample, \"inpainting_results/inpainted_grid_with_gen_img.png\", nrow=args.num_cols, normalize=True)\n",
        "            # Save grid of input images:\n",
        "            save_image(samples, \"inpainting_results/original_grid.png\", nrow=args.num_cols, normalize=True)\n",
        "            # Save grid of masked input images:\n",
        "            save_image(masked_samples, \"inpainting_results/masked_grid.png\", nrow=args.num_cols, normalize=True)\n",
        "            # Save grid of inpainted images:\n",
        "            save_image(filled_samples, \"inpainting_results/inpainted_images_grid.png\", nrow=args.num_cols, normalize=True)\n",
        "\n",
        "        # Get losses for each individual image (top to bottom, left to right):\n",
        "        for idx, (sample, filled_sample) in enumerate(zip(samples, filled_samples)):\n",
        "            image_idx = batch_idx * args.batch_size + idx\n",
        "\n",
        "            # Reshape into BxCxHxW (B = 1 since the batch size is a single image here)\n",
        "            # This is needed for torch SSIM loss\n",
        "            sample = sample.unsqueeze(0)\n",
        "            filled_sample = filled_sample.unsqueeze(0)\n",
        "\n",
        "            # Calculate losses for the current image:\n",
        "            L2_Loss = pixelwise_loss_L2(filled_sample, sample)\n",
        "            L1_Loss = pixelwise_loss_L1(filled_sample, sample)\n",
        "            PSNR_Loss = get_psnr(filled_sample, sample)\n",
        "            SSIM_Loss = SSIM_LossFn(filled_sample, sample) # get_ssim(filled_sample, sample)\n",
        "            #MS_SSIM_Loss = get_ms_ssim(filled_sample, sample)\n",
        "            print(\"==> Calculating Reconstruction Loss {}:\".format(image_idx))\n",
        "            print(\"L1: {0:.4f}\".format(L1_Loss.item()))\n",
        "            print(\"L2: {0:.4f}\".format(L2_Loss.item()))\n",
        "            print(\"PSNR: {0:.4f}\".format(PSNR_Loss.item())) # NOTE THAT PSNR AND L2 ARE RELATED\n",
        "            print(\"SSIM: {0:.4f}\".format(SSIM_Loss.item()))\n",
        "            #print(\"MS-SSIM: {0:.4f}\".format(MS_SSIM_Loss.item()))\n",
        "\n",
        "            total_L2 += L2_Loss.item()\n",
        "            total_L1 += L1_Loss.item()\n",
        "            total_PSNR += PSNR_Loss.item()\n",
        "            total_SSIM += SSIM_Loss.item()\n",
        "\n",
        "    # Calculate average loss:\n",
        "    mean_L2 = total_L2 / num_images\n",
        "    mean_L1 = total_L1 / num_images\n",
        "    mean_PSNR = total_PSNR / num_images\n",
        "    mean_SSIM = total_SSIM / num_images\n",
        "    print(\"==> Calculating average loss statistics:\")\n",
        "    print(\"Mean L1: {0:.4f}\".format(mean_L1))\n",
        "    print(\"Mean L2: {0:.4f}\".format(mean_L2))\n",
        "    print(\"Mean PSNR: {0:.4f}\".format(mean_PSNR))\n",
        "    print(\"Mean SSIM: {0:.4f}\".format(mean_SSIM))\n",
        "\n",
        "elif args.remove is None:\n",
        "    img = eval_transforms(img)\n",
        "    \"\"\"Mask center part of image\"\"\"\n",
        "    # Get upper-left pixel coordinate\n",
        "    i = (args.img_size - args.mask_size) // 2\n",
        "    masked_img = img.clone()\n",
        "    masked_img[:, i : i + args.mask_size, i : i + args.mask_size] = 1\n",
        "\n",
        "    # unsqueeze 0 to add extra dimension at position 0 (needed for generator)\n",
        "    # (Reshape to BxCxHxW)\n",
        "    img = Variable(img.type(Tensor)).unsqueeze(0)\n",
        "    masked_img = Variable(masked_img.type(Tensor)).unsqueeze(0)\n",
        "\n",
        "    # Generated the inpainted image:\n",
        "    gen_img = generator(masked_img)\n",
        "    gen_mask = gen_img[:, :, i : i + args.mask_size, i : i + args.mask_size]\n",
        "    inpainted_img = masked_img.clone()\n",
        "    inpainted_img[:, :, i : i + args.mask_size, i : i + args.mask_size] = gen_mask\n",
        "\n",
        "    # Calculate loss metrics between original image and output image:\n",
        "    L2_Loss = pixelwise_loss_L2(inpainted_img, img)\n",
        "    L1_Loss = pixelwise_loss_L1(inpainted_img, img)\n",
        "    PSNR_Loss = get_psnr(inpainted_img, img)\n",
        "    SSIM_Loss = SSIM_LossFn(inpainted_img, img)\n",
        "    print(\"==> Calculating Reconstruction Loss:\")\n",
        "    print(\"L1: {0:.4f}\".format(L1_Loss.item()))\n",
        "    print(\"L2: {0:.4f}\".format(L2_Loss.item()))\n",
        "    print(\"PSNR: {0:.4f}\".format(PSNR_Loss.item())) # NOTE THAT PSNR AND L2 ARE RELATED\n",
        "    print(\"SSIM: {0:.4f}\".format(SSIM_Loss.item()))\n",
        "\n",
        "    # print(\"==> Calculating Reconstruction Loss:\")\n",
        "    # for (name, pixelwise_loss) in loss_functions:\n",
        "    #     loss = pixelwise_loss(inpainted_img, img)\n",
        "    #     print(\"{}: {}\".format(name, loss))\n",
        "\n",
        "    # Save sample\n",
        "    sample = torch.cat((img.data, masked_img.data, inpainted_img.data), 0)\n",
        "    output_file = args.image_path.split(\"/\")[-1]\n",
        "    save_image(sample, \"inpainting_results/inpainted_%s\" % output_file, nrow=3, normalize=True) #nrow=3 since the sample has 3 images\n",
        "\n",
        "else: # If we are detecting / removing objects:\n",
        "\n",
        "    # Detect objects and extract mask:\n",
        "    img = Image.open(args.image_path)\n",
        "    img = np.array(img)\n",
        "    seg = Segmentor(args.image_path)\n",
        "    mask = seg.get_mask(args.remove)\n",
        "    if mask is None:\n",
        "        print(\"==> No objects detected. Exiting program.\")\n",
        "    else:\n",
        "        mask = mask.detach().cpu().numpy().astype(float)\n",
        "\n",
        "        # Resize image to mask size:\n",
        "        import cv2\n",
        "        mask_height, mask_width = mask.shape[:2]\n",
        "        resized_img = cv2.resize(img, (mask_width, mask_height), interpolation=cv2.INTER_CUBIC)\n",
        "        masked_img = resized_img.copy()\n",
        "        masked_img[mask != 0] = 255\n",
        "\n",
        "        # Convert back into PIL Images before applying torch transforms:\n",
        "        img = Image.fromarray(img)\n",
        "        mask = Image.fromarray(mask)\n",
        "        masked_img = Image.fromarray(masked_img)\n",
        "\n",
        "        # Apply transforms to input image and mask:\n",
        "        img = eval_transforms(img)\n",
        "        mask = mask_transforms(mask)\n",
        "        masked_img = eval_transforms(masked_img)\n",
        "\n",
        "        # Convert to Tensors and reshape from CxHxW to BxCxHxW:\n",
        "        img = Variable(img.type(Tensor)).unsqueeze(0)\n",
        "        mask = Variable(mask.type(Tensor)).unsqueeze(0)\n",
        "        masked_img = Variable(masked_img.type(Tensor)).unsqueeze(0)\n",
        "\n",
        "        # Generate image on masked image:\n",
        "        gen_img = generator(masked_img)\n",
        "\n",
        "        # Convert mask to have 3D channels:\n",
        "        mask = torch.cat((mask, mask, mask), dim=1)\n",
        "\n",
        "        # Find generated mask and inpaint this area onto a cloned version of the masked image:\n",
        "        mask_indices = (mask != 0).nonzero(as_tuple=True) # get new indices since they are tensors with extra dim?\n",
        "        gen_mask = gen_img[mask_indices]\n",
        "        inpainted_img = masked_img.clone()\n",
        "        inpainted_img[mask_indices] = gen_mask\n",
        "\n",
        "        # NOTE: Evaluating loss and reconstruction metrics won't work here\n",
        "        # since we have no ground truth image.\n",
        "\n",
        "        # Save sample:\n",
        "        output_file = args.image_path.split(\"/\")[-1]\n",
        "        sample = torch.cat((img.data, masked_img.data, inpainted_img.data), 0)\n",
        "        save_image(sample, \"inpainting_results/inpainted_%s\" % output_file, nrow=3, normalize=True)\n",
        "        sample = torch.cat((img.data, masked_img.data, gen_img.data, inpainted_img.data), 0)\n",
        "        save_image(sample, \"inpainting_results/inpainted_with_gen_img_%s\" % output_file, nrow=4, normalize=True)\n",
        "        save_image(mask.data, \"inpainting_results/inpainted_mask%s\" % output_file, nrow=1, normalize=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yT_hXurSahg",
        "outputId": "bbac1871-b11c-4523-cb92-4913468613f9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(image_folder_path='/content/sample_images', image_path='/content/sample_images_object_removal/beach.jpg', model_checkpoint='/content/Checkpoints/model_0.5L1_0.5SSIM_epoch_39.pth', remove=[0, 2], batch_size=-1, num_cols=-1, img_size=128, mask_size=64, channels=3)\n",
            "Number of input images: 64\n",
            "==> Batch size not specified. Using total number of images: 64\n",
            "==> Columns not specified. Using ceil(sqrt(num_images)) = 8\n",
            "==> Calculating Reconstruction Loss 0:\n",
            "L1: 0.0455\n",
            "L2: 0.0148\n",
            "PSNR: 18.2918\n",
            "SSIM: 0.7542\n",
            "==> Calculating Reconstruction Loss 1:\n",
            "L1: 0.0560\n",
            "L2: 0.0213\n",
            "PSNR: 16.7069\n",
            "SSIM: 0.7128\n",
            "==> Calculating Reconstruction Loss 2:\n",
            "L1: 0.0572\n",
            "L2: 0.0288\n",
            "PSNR: 15.4038\n",
            "SSIM: 0.7749\n",
            "==> Calculating Reconstruction Loss 3:\n",
            "L1: 0.0498\n",
            "L2: 0.0207\n",
            "PSNR: 16.8306\n",
            "SSIM: 0.7852\n",
            "==> Calculating Reconstruction Loss 4:\n",
            "L1: 0.0842\n",
            "L2: 0.0573\n",
            "PSNR: 12.4159\n",
            "SSIM: 0.7280\n",
            "==> Calculating Reconstruction Loss 5:\n",
            "L1: 0.0859\n",
            "L2: 0.0474\n",
            "PSNR: 13.2443\n",
            "SSIM: 0.7118\n",
            "==> Calculating Reconstruction Loss 6:\n",
            "L1: 0.0670\n",
            "L2: 0.0328\n",
            "PSNR: 14.8376\n",
            "SSIM: 0.7329\n",
            "==> Calculating Reconstruction Loss 7:\n",
            "L1: 0.0716\n",
            "L2: 0.0417\n",
            "PSNR: 13.7979\n",
            "SSIM: 0.7542\n",
            "==> Calculating Reconstruction Loss 8:\n",
            "L1: 0.0358\n",
            "L2: 0.0125\n",
            "PSNR: 19.0377\n",
            "SSIM: 0.8085\n",
            "==> Calculating Reconstruction Loss 9:\n",
            "L1: 0.0420\n",
            "L2: 0.0130\n",
            "PSNR: 18.8514\n",
            "SSIM: 0.7493\n",
            "==> Calculating Reconstruction Loss 10:\n",
            "L1: 0.0492\n",
            "L2: 0.0154\n",
            "PSNR: 18.1279\n",
            "SSIM: 0.7324\n",
            "==> Calculating Reconstruction Loss 11:\n",
            "L1: 0.0521\n",
            "L2: 0.0290\n",
            "PSNR: 15.3701\n",
            "SSIM: 0.8211\n",
            "==> Calculating Reconstruction Loss 12:\n",
            "L1: 0.0760\n",
            "L2: 0.0377\n",
            "PSNR: 14.2341\n",
            "SSIM: 0.7202\n",
            "==> Calculating Reconstruction Loss 13:\n",
            "L1: 0.0390\n",
            "L2: 0.0149\n",
            "PSNR: 18.2624\n",
            "SSIM: 0.8377\n",
            "==> Calculating Reconstruction Loss 14:\n",
            "L1: 0.0501\n",
            "L2: 0.0191\n",
            "PSNR: 17.1808\n",
            "SSIM: 0.7602\n",
            "==> Calculating Reconstruction Loss 15:\n",
            "L1: 0.0655\n",
            "L2: 0.0299\n",
            "PSNR: 15.2422\n",
            "SSIM: 0.7139\n",
            "==> Calculating Reconstruction Loss 16:\n",
            "L1: 0.0336\n",
            "L2: 0.0101\n",
            "PSNR: 19.9765\n",
            "SSIM: 0.7815\n",
            "==> Calculating Reconstruction Loss 17:\n",
            "L1: 0.0807\n",
            "L2: 0.0434\n",
            "PSNR: 13.6254\n",
            "SSIM: 0.7186\n",
            "==> Calculating Reconstruction Loss 18:\n",
            "L1: 0.0464\n",
            "L2: 0.0157\n",
            "PSNR: 18.0486\n",
            "SSIM: 0.7768\n",
            "==> Calculating Reconstruction Loss 19:\n",
            "L1: 0.0803\n",
            "L2: 0.0541\n",
            "PSNR: 12.6693\n",
            "SSIM: 0.7236\n",
            "==> Calculating Reconstruction Loss 20:\n",
            "L1: 0.0312\n",
            "L2: 0.0104\n",
            "PSNR: 19.8385\n",
            "SSIM: 0.8120\n",
            "==> Calculating Reconstruction Loss 21:\n",
            "L1: 0.0652\n",
            "L2: 0.0268\n",
            "PSNR: 15.7170\n",
            "SSIM: 0.7229\n",
            "==> Calculating Reconstruction Loss 22:\n",
            "L1: 0.1049\n",
            "L2: 0.0676\n",
            "PSNR: 11.6983\n",
            "SSIM: 0.7052\n",
            "==> Calculating Reconstruction Loss 23:\n",
            "L1: 0.0691\n",
            "L2: 0.0405\n",
            "PSNR: 13.9280\n",
            "SSIM: 0.7313\n",
            "==> Calculating Reconstruction Loss 24:\n",
            "L1: 0.0558\n",
            "L2: 0.0223\n",
            "PSNR: 16.5180\n",
            "SSIM: 0.7299\n",
            "==> Calculating Reconstruction Loss 25:\n",
            "L1: 0.0777\n",
            "L2: 0.0510\n",
            "PSNR: 12.9281\n",
            "SSIM: 0.7199\n",
            "==> Calculating Reconstruction Loss 26:\n",
            "L1: 0.0579\n",
            "L2: 0.0234\n",
            "PSNR: 16.3076\n",
            "SSIM: 0.7587\n",
            "==> Calculating Reconstruction Loss 27:\n",
            "L1: 0.0681\n",
            "L2: 0.0404\n",
            "PSNR: 13.9362\n",
            "SSIM: 0.7512\n",
            "==> Calculating Reconstruction Loss 28:\n",
            "L1: 0.0682\n",
            "L2: 0.0295\n",
            "PSNR: 15.2950\n",
            "SSIM: 0.7362\n",
            "==> Calculating Reconstruction Loss 29:\n",
            "L1: 0.0559\n",
            "L2: 0.0323\n",
            "PSNR: 14.9079\n",
            "SSIM: 0.7739\n",
            "==> Calculating Reconstruction Loss 30:\n",
            "L1: 0.0507\n",
            "L2: 0.0229\n",
            "PSNR: 16.4109\n",
            "SSIM: 0.7683\n",
            "==> Calculating Reconstruction Loss 31:\n",
            "L1: 0.0587\n",
            "L2: 0.0283\n",
            "PSNR: 15.4842\n",
            "SSIM: 0.7579\n",
            "==> Calculating Reconstruction Loss 32:\n",
            "L1: 0.0578\n",
            "L2: 0.0245\n",
            "PSNR: 16.1097\n",
            "SSIM: 0.7303\n",
            "==> Calculating Reconstruction Loss 33:\n",
            "L1: 0.0565\n",
            "L2: 0.0252\n",
            "PSNR: 15.9877\n",
            "SSIM: 0.7358\n",
            "==> Calculating Reconstruction Loss 34:\n",
            "L1: 0.0764\n",
            "L2: 0.0394\n",
            "PSNR: 14.0505\n",
            "SSIM: 0.7368\n",
            "==> Calculating Reconstruction Loss 35:\n",
            "L1: 0.0782\n",
            "L2: 0.0495\n",
            "PSNR: 13.0503\n",
            "SSIM: 0.7185\n",
            "==> Calculating Reconstruction Loss 36:\n",
            "L1: 0.0669\n",
            "L2: 0.0289\n",
            "PSNR: 15.3979\n",
            "SSIM: 0.7197\n",
            "==> Calculating Reconstruction Loss 37:\n",
            "L1: 0.0841\n",
            "L2: 0.0496\n",
            "PSNR: 13.0467\n",
            "SSIM: 0.7301\n",
            "==> Calculating Reconstruction Loss 38:\n",
            "L1: 0.0764\n",
            "L2: 0.0396\n",
            "PSNR: 14.0279\n",
            "SSIM: 0.7197\n",
            "==> Calculating Reconstruction Loss 39:\n",
            "L1: 0.0448\n",
            "L2: 0.0205\n",
            "PSNR: 16.8840\n",
            "SSIM: 0.8065\n",
            "==> Calculating Reconstruction Loss 40:\n",
            "L1: 0.0761\n",
            "L2: 0.0394\n",
            "PSNR: 14.0413\n",
            "SSIM: 0.7125\n",
            "==> Calculating Reconstruction Loss 41:\n",
            "L1: 0.0744\n",
            "L2: 0.0400\n",
            "PSNR: 13.9811\n",
            "SSIM: 0.7345\n",
            "==> Calculating Reconstruction Loss 42:\n",
            "L1: 0.0607\n",
            "L2: 0.0260\n",
            "PSNR: 15.8559\n",
            "SSIM: 0.7234\n",
            "==> Calculating Reconstruction Loss 43:\n",
            "L1: 0.0841\n",
            "L2: 0.0462\n",
            "PSNR: 13.3533\n",
            "SSIM: 0.7160\n",
            "==> Calculating Reconstruction Loss 44:\n",
            "L1: 0.0776\n",
            "L2: 0.0477\n",
            "PSNR: 13.2185\n",
            "SSIM: 0.7596\n",
            "==> Calculating Reconstruction Loss 45:\n",
            "L1: 0.0734\n",
            "L2: 0.0464\n",
            "PSNR: 13.3350\n",
            "SSIM: 0.7479\n",
            "==> Calculating Reconstruction Loss 46:\n",
            "L1: 0.0529\n",
            "L2: 0.0196\n",
            "PSNR: 17.0692\n",
            "SSIM: 0.7679\n",
            "==> Calculating Reconstruction Loss 47:\n",
            "L1: 0.0900\n",
            "L2: 0.0548\n",
            "PSNR: 12.6139\n",
            "SSIM: 0.7280\n",
            "==> Calculating Reconstruction Loss 48:\n",
            "L1: 0.0519\n",
            "L2: 0.0220\n",
            "PSNR: 16.5660\n",
            "SSIM: 0.7624\n",
            "==> Calculating Reconstruction Loss 49:\n",
            "L1: 0.0408\n",
            "L2: 0.0181\n",
            "PSNR: 17.4336\n",
            "SSIM: 0.7984\n",
            "==> Calculating Reconstruction Loss 50:\n",
            "L1: 0.0608\n",
            "L2: 0.0344\n",
            "PSNR: 14.6344\n",
            "SSIM: 0.7786\n",
            "==> Calculating Reconstruction Loss 51:\n",
            "L1: 0.0634\n",
            "L2: 0.0251\n",
            "PSNR: 15.9951\n",
            "SSIM: 0.7156\n",
            "==> Calculating Reconstruction Loss 52:\n",
            "L1: 0.0533\n",
            "L2: 0.0191\n",
            "PSNR: 17.1805\n",
            "SSIM: 0.7280\n",
            "==> Calculating Reconstruction Loss 53:\n",
            "L1: 0.0291\n",
            "L2: 0.0094\n",
            "PSNR: 20.2522\n",
            "SSIM: 0.7972\n",
            "==> Calculating Reconstruction Loss 54:\n",
            "L1: 0.0915\n",
            "L2: 0.0543\n",
            "PSNR: 12.6535\n",
            "SSIM: 0.7365\n",
            "==> Calculating Reconstruction Loss 55:\n",
            "L1: 0.0563\n",
            "L2: 0.0253\n",
            "PSNR: 15.9686\n",
            "SSIM: 0.7643\n",
            "==> Calculating Reconstruction Loss 56:\n",
            "L1: 0.0267\n",
            "L2: 0.0058\n",
            "PSNR: 22.3667\n",
            "SSIM: 0.8091\n",
            "==> Calculating Reconstruction Loss 57:\n",
            "L1: 0.0493\n",
            "L2: 0.0178\n",
            "PSNR: 17.5003\n",
            "SSIM: 0.7629\n",
            "==> Calculating Reconstruction Loss 58:\n",
            "L1: 0.0544\n",
            "L2: 0.0213\n",
            "PSNR: 16.7211\n",
            "SSIM: 0.7509\n",
            "==> Calculating Reconstruction Loss 59:\n",
            "L1: 0.0616\n",
            "L2: 0.0253\n",
            "PSNR: 15.9634\n",
            "SSIM: 0.7120\n",
            "==> Calculating Reconstruction Loss 60:\n",
            "L1: 0.0866\n",
            "L2: 0.0551\n",
            "PSNR: 12.5916\n",
            "SSIM: 0.7320\n",
            "==> Calculating Reconstruction Loss 61:\n",
            "L1: 0.0396\n",
            "L2: 0.0123\n",
            "PSNR: 19.0984\n",
            "SSIM: 0.7565\n",
            "==> Calculating Reconstruction Loss 62:\n",
            "L1: 0.0558\n",
            "L2: 0.0230\n",
            "PSNR: 16.3778\n",
            "SSIM: 0.7646\n",
            "==> Calculating Reconstruction Loss 63:\n",
            "L1: 0.0785\n",
            "L2: 0.0380\n",
            "PSNR: 14.1985\n",
            "SSIM: 0.7105\n",
            "==> Calculating average loss statistics:\n",
            "Mean L1: 0.0619\n",
            "Mean L2: 0.0306\n",
            "Mean PSNR: 15.6664\n",
            "Mean SSIM: 0.7489\n"
          ]
        }
      ]
    }
  ]
}